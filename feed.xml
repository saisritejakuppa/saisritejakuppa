<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://saisritejakuppa.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://saisritejakuppa.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-07T04:15:34+00:00</updated><id>https://saisritejakuppa.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">ECCV 2024</title><link href="https://saisritejakuppa.github.io/blog/2024/ECCV2024/" rel="alternate" type="text/html" title="ECCV 2024"/><published>2024-08-22T16:40:16+00:00</published><updated>2024-08-22T16:40:16+00:00</updated><id>https://saisritejakuppa.github.io/blog/2024/ECCV2024</id><content type="html" xml:base="https://saisritejakuppa.github.io/blog/2024/ECCV2024/"><![CDATA[<h2 id="-eccv-2024---my-experience-in-milan">üåç ECCV 2024 - My Experience in Milan</h2> <p>The European Conference on Computer Vision (ECCV) is one of the most prestigious events in the field of computer vision, held annually in Europe. This year, it was hosted in the charming city of Milan, Italy üáÆüáπ, and I had the incredible opportunity to be part of it!</p> <p>In this blog, I‚Äôll be sharing my experiences at ECCV 2024, and of course, showcasing beautiful moments captured in Italy. üì∏</p> <hr/> <h2 id="-eccv-conference-insights">üéâ <strong>ECCV Conference Insights</strong></h2> <h2 id="Ô∏è-virtual-try-on-workshop">üñ•Ô∏è <strong>Virtual Try-On Workshop</strong></h2> <p><strong>üìÑ Published Paper on DIVA</strong><br/> At ECCV 2024, I had the honor of presenting my paper, DIVA: Deep Indic Virtual Apparel Try-On, a project that holds deep personal and professional significance. As the world of virtual try-ons continues to evolve, the focus of our work was to tackle a niche yet crucial problem: virtual try-ons for Indic clothing.</p> <p>The journey began with recognizing a gap in existing virtual try-on systems ‚Äî most were not designed to handle the diversity and cultural richness of Indian garments. From the graceful sarees to the elaborate lehengas, these garments have unique shapes, drapes, and fabrics that present significant challenges for virtual modeling.</p> <p>To solve this, we introduced the IndicViton Dataset: a carefully curated collection of high-resolution images (720 √ó 540) that showcase a variety of Indian apparel in multiple poses and orientations. This dataset plays a critical role in training models that can capture the nuances of these garments.</p> <p>The heart of our work lies in the DIVA model ‚Äî a diffusion-based framework that handles the complexities of multi-pose garment images with remarkable visual accuracy. With DIVA, we could ensure that the virtual try-ons not only look realistic but also retain the cultural authenticity of the attire.</p> <p>At the heart of the paper is a recognition that the world of virtual try-ons must evolve to better reflect the cultural and regional diversity of clothing across the globe. By focusing on Indian fashion, our work hopes to make virtual clothing experiences more inclusive and accurate, catering to the unique needs of the Indian market and beyond.</p> <p><strong>Key Highlights</strong>:</p> <ul> <li><strong>IndicViton Dataset</strong>: A high-resolution (720 √ó 540) dataset featuring garments in multiple poses and orientations, helping achieve better results in virtual try-ons.</li> <li><strong>Diffusion-Based Model (DIVA)</strong>: A cutting-edge model that handles multi-pose garment images with high visual fidelity, ensuring accurate virtual try-on outcomes.</li> <li><strong>Culturally Specific Focus</strong>: Tailored to the diverse Indian apparel, including dhoti, sarees, kurtas, lehengas, and more. Usage of scribble maps as priors to achieve this.</li> </ul> <p><strong>Publication</strong><br/> The research was presented at <strong>ECCV 2024</strong>.<br/> üëâ <a href="https://saisriteja.github.io/DIVA/">Access the paper here</a></p> <hr/> <h3 id="-networking-with-industry-peers">ü§ù <strong>Networking with Industry Peers</strong></h3> <p>I had the opportunity to connect with professionals from <strong>Amazon</strong> and <strong>TCS</strong>, who are working on virtual try-on solutions for the Indian market. We exchanged ideas, discussed challenges, and explored the growing potential in the virtual try-on space.</p> <hr/> <h3 id="-global-insights">üåè <strong>Global Insights</strong></h3> <p>I also interacted with researchers from <strong>Korea</strong> and <strong>China</strong>, learning about their innovative methods in virtual try-ons. It was incredible to get a glimpse of their approaches and gain insights into global advancements in this exciting field.</p> <h3 id="-eccv-photos">üì∏ <strong>ECCV Photos</strong></h3> <p>Check out some of my favorite moments from ECCV 2024 below!</p> <div align="center"> <a href="https://drive.google.com/file/d/1repEITw0AkNDYHVH489ikjR78YQ6QZXE/view?usp=sharing"> <img src="https://drive.google.com/thumbnail?id=1repEITw0AkNDYHVH489ikjR78YQ6QZXE&amp;sz=w1000" alt="ECCV Workshop Picture" style="max-width: 100%; height: auto;"/> </a> </div> <div align="center"> üì∏ **ECCV Workshop Pictures** </div> <hr/> <h2 id="insights-on-academic-and-research-success">Insights on Academic and Research Success</h2> <ol> <li> <p><strong>Focus on Quality over Quantity</strong><br/> Aim to write impactful, high-quality papers, following the example of researchers like <strong>Kaiming He</strong>, rather than producing a high volume of low-impact work.</p> </li> <li> <p><strong>Future-Forward Problem Statements</strong><br/> Select research problems that are ahead of their time, focusing on areas that the industry is unlikely to tackle in the next 5 years. This ensures your work remains <strong>relevant</strong> and <strong>groundbreaking</strong>.</p> </li> <li> <p><strong>Iterative Adaptation in Research</strong><br/> Approach research topics like developing a taste for coffee‚Äîinitially challenging, but over time, you‚Äôll adapt to a particular flavor or niche that resonates with you.</p> </li> <li> <p><strong>Deep Expertise in a Single Area</strong><br/> Develop deep expertise in one domain rather than spreading efforts thinly across multiple topics. <strong>Specialization</strong> often yields greater recognition and deeper contributions.</p> </li> <li> <p><strong>Fundamental Innovations Over Trends</strong><br/> Focus on addressing <strong>fundamental challenges</strong> rather than following hyped topics. Unique, foundational contributions often yield long-term impact and recognition.</p> </li> <li> <p><strong>Admission Processes</strong><br/> Understand that professors may not have complete autonomy in selecting students; initial screenings and decisions are often made at the university level based on set criteria.</p> </li> <li> <p><strong>Conferences as Networking Hubs</strong><br/> Attending conferences like <strong>ECCV</strong> not only helps in sharing your work but also offers opportunities to <strong>network</strong> with leading researchers, identify trending topics, and gather feedback to refine your research direction.</p> </li> <li> <p><strong>Industry-Academia Divide</strong><br/> Recognize that academia allows freedom to explore novel, uncharted problems, while <strong>industry research</strong> may focus on immediate applications and profitability. Use this distinction to define your <strong>academic contributions</strong>.</p> </li> <li> <p><strong>Patience in Research Journey</strong><br/> Breakthroughs often take time. Commit to your research journey with <strong>patience</strong>, <strong>persistence</strong>, and <strong>adaptability</strong>, as impactful results may not come immediately.</p> </li> </ol> <div align="center"> <a href="https://drive.google.com/file/d/1p-HuvAbKQZwS-KiABASngaJ36Itp4eLw/view?usp=sharing"> <img src="https://drive.google.com/thumbnail?id=1p-HuvAbKQZwS-KiABASngaJ36Itp4eLw&amp;sz=w1000" alt="ECCV Workshop Picture" style="max-width: 100%; height: auto;"/> </a> </div> <div align="center"> Prof. Michel J. Black (left) and Prof. Shree Nayar (right) </div> <hr/> <h2 id="what-to-do-at-a-conference">What to Do at a Conference</h2> <ol> <li> <p><strong>Prepare by Reviewing Papers</strong><br/> Prior to attending, obtain the list of presented papers and review their work in advance. This preparation helps you identify specific researchers or sessions of interest, allowing you to ask insightful questions in person. Avoid going in unprepared, as the abundance of information can be overwhelming and lead to quick saturation. Expect a block of papers related to <strong>3D</strong>, <strong>2D</strong>, <strong>diffusion models</strong>, <strong>federated learning</strong>, etc.</p> </li> <li> <p><strong>Build a Strong Network</strong><br/> Make as many connections as possible! Take photos of attendee ID cards to remember individuals and later connect with them. Engage in conversations to learn more about their research and foster potential collaborations. I had a chance to talk with academic cousins, professors, and their students.</p> </li> <li> <p><strong>Attend Workshops Strategically</strong><br/> Workshops often provide a condensed overview of state-of-the-art techniques and ideas within an hour. Attending these sessions allows you to absorb key advancements efficiently. Since I am more of a <strong>2D vision</strong> guy, I would be interested in <strong>3D vision</strong> and solving <strong>3D problems in 2D</strong>.</p> </li> <li> <p><strong>Participate in Poster Sessions</strong><br/> Poster sessions are excellent for one-on-one interactions with researchers. Use this opportunity to delve deeper into their work, discuss methodologies, and explore potential applications of their research.</p> </li> <li> <p><strong>Engage in Q&amp;A Sessions</strong><br/> Ask questions during presentations to clarify concepts, challenge ideas, or spark discussions. This not only helps you understand the work better but also establishes you as an engaged participant.</p> </li> <li> <p><strong>Take Detailed Notes</strong><br/> Keep a notebook or digital device handy to jot down key takeaways, interesting ideas, or potential future research directions. Notes will help you revisit and reflect on what you‚Äôve learned after the conference.</p> </li> <li> <p><strong>Exchange Ideas</strong><br/> Share your work with peers to get feedback and suggestions. Conversations about your research can lead to new insights or even collaborations.</p> </li> <li> <p><strong>Follow Up Post-Conference</strong><br/> After the conference, reach out to the people you met. This follow-up helps solidify connections and keeps the conversation alive for potential collaborations or mentorship. This is where I make most of my connections‚Äîthe last day is just for that! Say hello to as many people as you can.</p> </li> <li> <p><strong>Explore Industry Stalls</strong><br/> If the conference includes industry exhibits, visit booths to learn about the latest tools, datasets, or collaborations that could enhance your research. You can also ask people about referrals or problems they are trying to solve.</p> </li> <li> <p><strong>Plan Breaks and Downtime</strong><br/> Conferences can be intense, so schedule short breaks to rest and process information. Use this time to review notes, organize contacts, or prepare for upcoming sessions.</p> </li> <li> <p><strong>Attend Keynotes and Plenaries</strong><br/> Keynote speeches often provide a broad perspective on current trends and future directions in the field. Make it a priority to attend these sessions.</p> </li> </ol> <hr/> <h3 id="-visiting-nearby-places">üåç <strong>Visiting Nearby Places</strong></h3> <div align="center"> <a href="https://drive.google.com/file/d/1a8-6INTbHdH_yR6g8Fi6JAf21XJxkPze/view?usp=sharing"> <img src="https://drive.google.com/thumbnail?id=1a8-6INTbHdH_yR6g8Fi6JAf21XJxkPze&amp;sz=w1000" alt="Milan" style="max-width: 100%; height: auto;"/> </a> </div> <div align="center"> Milan </div> <div align="center"> <a href="https://drive.google.com/file/d/1ZKO2bH4v0LyUh3OZBGCVvuzu1XZb6GmY/view?usp=sharing"> <img src="https://drive.google.com/thumbnail?id=1ZKO2bH4v0LyUh3OZBGCVvuzu1XZb6GmY&amp;sz=w1000" alt="Rome" style="max-width: 100%; height: auto;"/> </a> </div> <div align="center"> Rome </div>]]></content><author><name></name></author><category term="Conference"/><category term="ECCV"/><category term="Personal"/><summary type="html"><![CDATA[üåç ECCV 2024 - My Experience in Milan]]></summary></entry><entry><title type="html">Better Code Part I</title><link href="https://saisritejakuppa.github.io/blog/2023/BetterCodeSeriesI/" rel="alternate" type="text/html" title="Better Code Part I"/><published>2023-08-22T16:40:16+00:00</published><updated>2023-08-22T16:40:16+00:00</updated><id>https://saisritejakuppa.github.io/blog/2023/BetterCodeSeriesI</id><content type="html" xml:base="https://saisritejakuppa.github.io/blog/2023/BetterCodeSeriesI/"><![CDATA[<h1 id="good-names-in-code">Good Names in Code</h1> <p>Names are meant to be meaningful, should understand what it contains without knowing whats happening behind the scenario.</p> <p>This you will be naming often</p> <ol> <li>variable/contants <ol> <li>Contains dataset, results of a function‚Ä¶ anything - use Nouns(userdata) or shortphrases with adjectives(isValid)</li> </ol> </li> <li>function/methods <ol> <li>Use verbs(sendData) or short phrases with adjectives(isInputValid)</li> </ol> </li> <li>classes <ol> <li>These are used to make an object - use Nouns or shortPhrases(User, RequestBody)</li> </ol> </li> </ol> <h2 id="naming-convections">Naming convections</h2> <ol> <li>SnakeCase - hello_world(python - variables, function/methods)</li> <li>camelCase - helloWorld( java or script - variables, function/methods)</li> <li>pascalCase - HelloWorld(python/java - Classes)</li> </ol> <h3 id="varaible-constants-and-properties">Varaible, Constants and Properties</h3> <ol> <li>Values is an object. <ol> <li>Describe the object <ol> <li>User</li> <li>database</li> </ol> </li> <li>Provide More information of the variable <ol> <li>Authentic User</li> <li>Sql Database</li> </ol> </li> </ol> </li> <li>Value can be boolean. <ol> <li>Tell True or False <ol> <li>isValid</li> <li>loggedIn</li> </ol> </li> <li>Provide More information of the variable <ol> <li>isUserLoggedIN</li> </ol> </li> </ol> </li> <li>Value can be number or string. <ol> <li>Describe the value <ol> <li>name</li> <li>age</li> </ol> </li> <li>Provide More information of the variable <ol> <li>firstname</li> <li>age</li> </ol> </li> </ol> </li> </ol> <h3 id="functions">Functions</h3> <ol> <li>Functions perform an operation <ol> <li>Describe the operation <ol> <li>GetUserInfo</li> <li>Response.send()</li> </ol> </li> </ol> </li> <li>Computes a boolean <ol> <li>Know about the status <ol> <li>isValid()</li> <li>Purchase.isPaid() Also you can add more information about to the name like EmailResponse.send(), this tells you are working on some email responding operation.</li> </ol> </li> </ol> </li> </ol> <h3 id="classes">Classes</h3> <ol> <li>Describe the Object <ol> <li>UserProduct- it can become customer or course.</li> </ol> </li> </ol> <h2 id="tips">Tips</h2> <ol> <li>Be consistent about the code names.( use get over fetch if you like it.)</li> <li>Dont use bad language.</li> <li>Dont overadd or under add the required info requried for the variable.</li> </ol> <h1 id="comments">Comments</h1> <h2 id="bad-comments">Bad comments</h2> <ol> <li>Giving redundant information.</li> <li>The variable names and the comments give different information.</li> <li>Large comments that will block the code.</li> <li>Commented code is scary in production and can lead to misleading information. Use version control system to bring the old code and delete the bad code.</li> </ol> <h2 id="good-comments">Good Comments</h2> <ol> <li>Legal Information</li> <li>Explanation that cant be delivered by variable names.</li> <li>Important warnings while running the code.</li> <li>Import docstring that are essential for API.</li> </ol> <h2 id="code-formatting">Code formatting</h2> <ol> <li>Vertical formatting <ol> <li>Spacing of lines</li> <li>Grouping of lines</li> </ol> </li> <li>Horizontal formatting <ol> <li>intendation</li> <li>width of code lines.</li> <li>Space between the code.</li> </ol> </li> </ol> <h3 id="vertical-formatting">Vertical Formatting</h3> <ol> <li>Multiple classes in a single file, split it into multiple files.</li> <li>Adding lines between the codes, we have autoformatters to do that. Similar concepts shouldnt be seperatad by spaces.</li> <li>Stack the function in a proper classes, so the search becomes easy.</li> </ol> <h3 id="horizontal-formatting">Horizontal Formatting</h3> <ol> <li>Lines should be readable without even scrolling.</li> <li>Long lines can be done multiple shorter ones.</li> <li>Dont use longer variable names, so the length of the line is shorter to read.</li> </ol>]]></content><author><name></name></author><category term="BetterCode"/><category term="SoftwareTools"/><summary type="html"><![CDATA[Good Names in Code]]></summary></entry><entry><title type="html">DeepSea Series</title><link href="https://saisritejakuppa.github.io/blog/2023/DeepSea/" rel="alternate" type="text/html" title="DeepSea Series"/><published>2023-08-22T16:40:16+00:00</published><updated>2023-08-22T16:40:16+00:00</updated><id>https://saisritejakuppa.github.io/blog/2023/DeepSea</id><content type="html" xml:base="https://saisritejakuppa.github.io/blog/2023/DeepSea/"><![CDATA[<blockquote> <p>this is under development and is updated continously.</p> </blockquote> <h1 id="youtube-series-for-deep-sea">Youtube Series for Deep Sea</h1> <p>cuttle fish hypnosis - https://www.youtube.com/watch?v=rbDzVzBsbGM&amp;pp=ygUSZGVlcCBzZWEgYmNjIGVhcnRo</p>]]></content><author><name></name></author><category term="DeepSea"/><category term="Personal"/><summary type="html"><![CDATA[this is under development and is updated continously.]]></summary></entry><entry><title type="html">SignLangauge - A Review of entire system.</title><link href="https://saisritejakuppa.github.io/blog/2023/SignLanguageResearch/" rel="alternate" type="text/html" title="SignLangauge - A Review of entire system."/><published>2023-08-22T16:40:16+00:00</published><updated>2023-08-22T16:40:16+00:00</updated><id>https://saisritejakuppa.github.io/blog/2023/SignLanguageResearch</id><content type="html" xml:base="https://saisritejakuppa.github.io/blog/2023/SignLanguageResearch/"><![CDATA[<h1 id="introduction-to-sign-language">Introduction to Sign Language</h1> <p>Signers communicate with peers using hand signs. Thus we are require of systems that can help us to convert these signs to words and inverse as well.</p> <p>There are 3 different things in sign langauge world.</p> <ol> <li>Sign Language Recognition - Convert signs to words.</li> <li>Sign Language Generation - Convert words to signs.</li> <li>Sign Language Translation - Convert signs to words and words to signs.</li> </ol> <p>All the datasets in the sign langauge would have a video + gloss + text. Based on the dataset availability we procced for the type of the task. A lot of effort is put in the sign language recognition. I was more of a CV engineer, I did like the generation part. My goal is to make a system where an user enters the text input and the system generates the video of the sign language. The output video is meant to be a human realistic sign language video.</p> <p>The things I am intrested in to look for are:</p> <ol> <li>Text to Pose</li> <li>Text to Video</li> </ol> <p>Anyways in both the procedure the common ground is to have text input and a video output. When I say text, the input would be a spoken langauge like english, hindi, french‚Ä¶etc. The output would be a video of the sign language( skeleton based one or a human realistic one).</p> <h2 id="research-papers">Research Papers</h2> <h3 id="2023">2023</h3> <ol> <li>Ham2Pose: Animating Sign Language Notation into Pose Sequences.<a href="https://openaccess.thecvf.com/content/CVPR2023/html/Arkushin_Ham2Pose_Animating_Sign_Language_Notation_Into_Pose_Sequences_CVPR_2023_paper.html">paper</a></li> <li>Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation</li> <li>Co-speech Gesture Synthesis by Reinforcement Learning with Contrastive Pre-trained Rewards.</li> </ol> <h3 id="2022">2022</h3> <ol> <li>BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis</li> <li>Audio-Driven Stylized Gesture Generation with Flow-Based Model.</li> <li>Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production.</li> <li>Audio-Driven Neural Gesture Reenactment With Video Motion Graphs.</li> <li>Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation.</li> <li>SEEG: Semantic Energized Co-Speech Gesture Generation.</li> <li>Low-Resource Adaptation for Personalized Co-Speech Gesture Generation.</li> <li>Learning Unseen Emotions from Gestures via Semantically-Conditioned Zero-Shot Perception with Adversarial Autoencoders</li> <li>Text2Sign: Towards Sign Language Production Using Neural Machine Translation and Generative Adversarial Networks.</li> </ol> <h3 id="2021">2021</h3> <ol> <li>Speech Drives Templates: Co-Speech Gesture Synthesis With Learned Templates.</li> <li>Audio2Gestures: Generating Diverse Gestures From Speech Audio With Conditional Variational Autoencoders.</li> <li>Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives.</li> <li>Towards Fast and High-Quality Sign Language Production.</li> <li>Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning.</li> <li>Progressive Transformers for End-to-End Sign Language Production.</li> <li>Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional-Mixture Approach.</li> <li>Neural Sign Language Synthesis: Words Are Our Glosses.</li> </ol> <h3 id="2018">2018</h3> <ol> <li>Sign Language Production using Neural Machine Translation and Generative Adversarial Networks.</li> </ol>]]></content><author><name></name></author><category term="SignLangauge"/><category term="Projects"/><summary type="html"><![CDATA[Sign Language Video Generation.]]></summary></entry><entry><title type="html">Disfluency detection using deep learning.</title><link href="https://saisritejakuppa.github.io/blog/2022/DisfluencyDetection/" rel="alternate" type="text/html" title="Disfluency detection using deep learning."/><published>2022-09-22T16:40:16+00:00</published><updated>2022-09-22T16:40:16+00:00</updated><id>https://saisritejakuppa.github.io/blog/2022/DisfluencyDetection</id><content type="html" xml:base="https://saisritejakuppa.github.io/blog/2022/DisfluencyDetection/"><![CDATA[<h1 id="disfluency-detection-using-deep-learning">DisFluency Detection Using Deep Learning</h1> <p>Disfluency detection is the task of identifying and classifying disfluent speech segments in spoken language. It can be useful in many applications such as speech therapy, speech recognition and natural language processing. Deep learning methods have been used to detect disfluencies with high accuracy. These methods use neural networks, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), to analyze the acoustic and/or linguistic features of the speech signal. The neural network is trained on a dataset of labeled speech segments and then can be used to classify new speech segments as fluent or disfluent. The use of deep learning methods for disfluency detection has shown promising results and has the potential to improve the performance of disfluency detection systems.</p> <p>Softwares used:</p> <ol> <li>Python</li> <li>Praat</li> <li>TensorFlow</li> </ol> <p>PRATT is a software tool that is used for the purpose of speech processing. It is designed to help individuals with speech impairments, such as dysarthria, to improve their speech intelligibility. The software uses advanced speech processing algorithms to analyze the user‚Äôs speech and provide real-time feedback on various aspects of speech, such as pitch, loudness, and duration. Users can also practice specific speech exercises and track their progress over time. PRATT can be used in a clinical setting by speech therapists or can be used as a self-help tool by individuals with speech impairments. The software is typically used in conjunction with other speech therapy techniques and interventions. Overall, PRATT is a powerful tool that can help individuals with speech impairments to improve their speech intelligibility and communication skills.</p> <h1 id="complete-algorithm">Complete Algorithm</h1> <p align="center"> <img src="https://user-images.githubusercontent.com/48018142/163709544-7e40bb40-1b57-42ee-90c9-70397f79e71e.JPG" alt="ultrasoundgif" class="center"/> <br/> <a href="">Complete Algorithm.</a> </p> <h2 id="process-of-the-project">Process of the Project</h2> <ol> <li>Build a website and connect it to the AWS to save the recorded data which we get live.</li> <li>Data Analysis and Data Cleaning.</li> <li>Data Agumentations and Speech Processing tricks for a better output.</li> <li>Build a Deep Learning Model to Predict the disfluency in speech.</li> <li>Praat Software to get meta data.</li> </ol> <h1 id="1-website-building-and-connecting-to-github">1. Website Building and connecting to Github</h1> <p>We have build a simple website using bootstrap and HTML, basic javascript to do some operations in the website. Flask Package is used to deploy the website in Heroku. We have written script in such way that the audio collected is saved in AWS S3 bucket. Then we get to download the data from the bucket for further process. The website consits of a GUI to record speech data, the person is given a set of questions to explain, and is about speak about 3 mins regarding ques. This is recorded and stored. The link of the website is attached below.</p> <p>https://myprosody.herokuapp.com/</p> <p align="center"> <img src="https://user-images.githubusercontent.com/48018142/163707254-5e810fcd-d281-41db-a81a-6bb1b35e72f7.png" alt="ultrasoundgif" class="center"/> <br/> <a href="">Webpage for data collection.</a> </p> <h1 id="2-data-analysis-part">2. Data Analysis Part</h1> <ol> <li>The data we get is recorded from various devices and different browser extensions. So sampling rate is set accordingly for proper pre processing.</li> <li>Human Pitch for Men(100- 120Hz) and Women(300Hz), so low frequency information is necessary and high frequency is discarded. To do this we used a low pass filter.</li> <li>A window is 10 seconds is sampled so that we can send in limited features to detect disfluencies.</li> <li>The Mel scale mimics how the human ear works, with research showing humans don‚Äôt perceive frequencies on a linear scale. Humans are better at detecting differences at lower frequencies than at higher frequencies. So we have used mel spectrograms as an input.</li> </ol> <p>Librosa is a python library for analyzing and manipulating audio files. It is designed to be easy to use, and provides a wide range of functionality for tasks such as feature extraction, audio segmentation, and audio visualization. Some of the key features of librosa include: -Support for a wide range of audio file formats and codecs -Tools for loading and resampling audio data -Tools for audio feature extraction, including Mel-frequency cepstral coefficients (MFCCs), chroma, and tempo estimation -Tools for audio segmentation, such as beat tracking, onset detection, and pitch estimation -Tools for audio visualization, including waveform plots, spectrograms, and tonnetz plots -Integration with other popular python libraries such as numpy, scipy and matplotlib.</p> <p>Librosa is widely used in the field of music information retrieval and audio processing, and is a popular choice for researchers and developers working in these areas. It is open source and actively maintained.</p> <p>I have used the Librosa library to extract the spectrograms and do the augumentations.</p> <p align="center"> <img src="https://user-images.githubusercontent.com/48018142/163707377-24d26e11-ce0a-4934-90fb-4d64911ea4af.JPG" alt="ultrasoundgif" class="center"/> <br/> <a href="">Data Collection Workflow.</a> </p> <p align="center"> <img src="https://user-images.githubusercontent.com/48018142/163709124-d00760eb-f70a-4c74-ab29-eb55882fdb7c.JPG" alt="ultrasoundgif" class="center"/> <br/> <a href="">Generating Spectrogram.</a> </p> <h1 id="3-data-agumentations">3. Data Agumentations</h1> <p>Data augmentation is a technique used to artificially increase the size of a dataset in order to improve the performance of machine learning models. One common data augmentation technique for audio data is to black out pixels in the spectrograms. Spectrograms are visual representations of audio data, where the x-axis represents time, the y-axis represents frequency, and the intensity of each pixel represents the amplitude of the audio signal at that point in time and frequency. By randomly blacking out pixels in the spectrograms, the model is forced to learn to be robust to missing information, which can improve its ability to generalize to new unseen data.</p> <p>This technique can be implemented by applying a mask to the spectrogram, where certain pixels are randomly set to zero. The mask can be applied with different probability, and can be applied in a localized way such as blacking out a random square region of the spectrogram. This can be done using python library such as numpy, scipy and librosa.</p> <p>Blacking out pixels in the spectrograms can be especially effective when combined with other data augmentation techniques, such as time shifting, pitch shifting, and adding noise to the audio signal. By applying these different types of data augmentation, the model can learn to be robust to a wide range of variations in the input data, which can lead to improved performance.</p> <p align="center"> <img src="https://user-images.githubusercontent.com/48018142/163709280-9a2191fa-a436-44d8-b980-4f325bef81cf.JPG" alt="ultrasoundgif" class="center"/> <br/> <a href="">Data Augumentations.</a> </p> <h1 id="4-deep-learning-architecture">4. Deep Learning Architecture</h1> <p>The model consist of resnet blocks to observe patterns in spectrograms and predict output. The output can predict both the labels, since we have used multilabel classification using sigmoid at the end for each classifier.</p> <p>The CNN architecture is designed to be able to learn patterns in the spectrograms, which can then be used to make predictions about the audio data. The specific architecture used in this model is a variant of the ResNet blocks.</p> <p>ResNet blocks are a type of building block used in CNNs that are designed to be able to learn residual representations of the input data. This means that the model is able to learn the differences between the input data and a set of learned features, rather than trying to learn the features themselves. This can make the model more robust and able to generalize better to new unseen data.</p> <p>In this model, the ResNet blocks are used to observe patterns in the spectrograms, and the output of the model is used to make predictions about the audio data. The model is trained using a multilabel classification task, which means that it is able to predict multiple labels for a given input. The final output layer of the model uses a sigmoid activation function for each classifier, which allows the model to output a probability for each label.</p> <p>The use of ResNet blocks and a multilabel classification task allows the model to make predictions about the audio data with high accuracy. Additionally, the use of a sigmoid activation function in the final output layer allows the model to output probabilities for each label, which can be useful for making decisions based on the model‚Äôs predictions.</p> <p align="center"> <img src="https://user-images.githubusercontent.com/48018142/163709308-5bcf16f9-f2b0-4eef-bee3-2151a9f492a4.png" alt="ultrasoundgif" class="center"/> <br/> <a href="">Model Architecture.</a> </p> <h1 id="5-additional-data">5. Additional Data</h1> <p>Certain audio clips contain no information and noise, for such data using a deep learning model is a waste of computation, so we use praat software to detect such things and help it to predict as long pauses. The algorithm is good enough to predict all the filler long pauses and is highly accurate. Hence it can deal in all the situations.</p> <h1 id="results">Results</h1> <p align="center"> <img src="https://user-images.githubusercontent.com/48018142/163709282-f0ddeffc-2933-4d6b-83a3-c977bcf5e93e.JPG" alt="ultrasoundgif" class="center"/> <br/> <a href="">Results.</a> </p>]]></content><author><name></name></author><category term="SpeechProcessing"/><category term="SpeechProcessing"/><summary type="html"><![CDATA[A blog about disfluency detection using deep learning.]]></summary></entry><entry><title type="html">Nvidia Tools</title><link href="https://saisritejakuppa.github.io/blog/2022/NvidiaTools/" rel="alternate" type="text/html" title="Nvidia Tools"/><published>2022-09-22T16:40:16+00:00</published><updated>2022-09-22T16:40:16+00:00</updated><id>https://saisritejakuppa.github.io/blog/2022/NvidiaTools</id><content type="html" xml:base="https://saisritejakuppa.github.io/blog/2022/NvidiaTools/"><![CDATA[<h1 id="nvidia-tao-tutorial">Nvidia Tao Tutorial</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepstream/ds2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepstream/ds2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepstream/ds2-1400.webp"/> <img src="/assets/img/deepstream/ds2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> why DeepStream in the first place? </div> <h2 id="introduction">Introduction</h2> <p><strong>Decode Input source:</strong> OpenCV is a go to choice for operations on images. It is a library of programming functions mainly aimed at real-time computer vision. It is used for all sorts of image and video analysis, like facial recognition and detection, license plate reading, photo editing, advanced robotic vision, optical character recognition, and a whole lot more.</p> <p><strong>Run Inference:</strong> Pytorch for researchers and Tensorflow for production. this is the brain of the system. It is a deep learning framework that is used to build and train neural networks. It is a free and open-source software library for data manipulation and analysis. It is used for a wide range of tasks including data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, computer vision, and natural language processing.</p> <p><strong>Logic Block:</strong> its the logical code you write to connect things, mostly use python since its easy to learn and implement stuff.</p> <p><strong>Output:</strong> Displays, IOT services, AWS Servers could be anything.</p> <p>One things is none of the above are accelerated except the inference block. This is because a module does operations on CPU and a few does it on GPU, the bottleneck part takes time and makes your system slow.</p> <h2 id="deepstream-module">Deepstream Module</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepstream/ds3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepstream/ds3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepstream/ds3-1400.webp"/> <img src="/assets/img/deepstream/ds3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Blocks of Deepstream Module </div> <p>CCTV generate feed called RTSP, sends data over that protocol, networks operations are done on CPU Decoders generate the information, images.jpg to numpy arrary for pythonists using OpenCV. Image processing stuff, like cropping, resizing, etc are done on CPU/GPU, this is where the bottleneck is. Batching is always done on gpu. DNN( deep neural nets) are done on GPU/ DLA(deep learning accelerator) which is a part of GPU. DLA is similar to TPU. Tracking for object detection is done on GPU/CPU. visuvalization, When you wnated to draw on an image, GPU takes care of it.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepstream/ds1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepstream/ds1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepstream/ds1-1400.webp"/> <img src="/assets/img/deepstream/ds1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Complete Deepstream Module </div> <p>One of the main module that slows down the system is the detector/ classifier, due to no of matrix operations that it has to do on the GPU, we need a better way to do it. This is where Nvidia Tao and TensorRT comes in.</p> <h2 id="tensorrt">TensorRT</h2> <p>TensorRT is the heart of the Deepstream. You can take TRTorch to convert your pytorch model to TensorRT. TensorRT is a platform for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime that delivers low latency and high-throughput for deep learning inference applications. TensorRT is a C++ library with Python bindings that allows developers to maximize GPU utilization and extract maximum performance from NVIDIA GPUs. TensorRT provides a Python API that allows developers to integrate deep learning inference into their applications. TensorRT is available in the NVIDIA Deep Learning SDK and is supported on NVIDIA Jetson, NVIDIA DGX, and NVIDIA T4 GPUs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepstream/ds4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepstream/ds4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepstream/ds4-1400.webp"/> <img src="/assets/img/deepstream/ds4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> TensorRT Module </div> <p><strong>Precision Calibration:</strong> for Medical Imaging use fp64. for almost all other applications you can degrade it fp16 or int8, so that we can run the model at higher fps</p> <p><strong>Layer and Tensor:</strong> Advanxed tech that can do matrix mutiplications at a faster pace, this is where the speed comes from. I am talking about operations like convolutions, max pooling and, stuff like that in deep learning, incase if you find a new operation you can even write one of your own in tensorrt OSS and achieve the same performance the nvidia gives you as a researcher or a hobbyist.</p> <p><strong>Dynamic Tensor Memory:</strong> One need to clean the cache memory once done with operations, this is done by tensorRT, so that you dont have to worry about it.</p> <p><strong>MultiStream Execution:</strong> naive pytorch/tf does have a capability of handling the parallel streams together, nvidia does that for us, so the performace remains almost same all the time.</p> <p><strong>Kernal autotuning</strong> GPU have kernals, these kernals are used to do operations on the GPU, these kernals are optimized for the operations that we do, so that we can achieve the best performance. The engine file is generated based on the gpu you use, so that it can give the best perfomance.</p> <p>TensorRT makes an engine file in the end which is used by Deepstream to run the inference. Sometime its a tedious process to install things and check, because you can train on an VM( which can have a A100 GPU) and do inference of sample images on local machine GTX series, errors can rise due to different CUDA versions, different TensorRT versions, etc. Deepstream have bindings inside which can do this for you, by giving either a .onnx file or a weights file.</p> <h2 id="gstreamer">GSTreamer</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepstream/ds6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepstream/ds6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepstream/ds6-1400.webp"/> <img src="/assets/img/deepstream/ds6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Gstreamer Module </div> <p>Deepstream gives certain level plugin, lets say you have an image of (254,512,3) and wanted to resize it to(224,224,3), the plugin is loaded and the operation is done in GPU using deeplearning.</p> <p>Every plugin have a src(source) and a snk(sink). The function of them are analogous to input/output. People like to write code modular, so you make things in sequence.</p> <p><strong>Bin</strong> is a sequence of plugins. <strong>Pipeline</strong> is a sequence of bins. <strong>Pipelines</strong> can be used to deal and control things according to the user.</p> <h2 id="triton-server">Triton Server</h2> <p>This is better of all for some reason, if the models are not compactable to deepstream.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deepstream/ds5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deepstream/ds5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deepstream/ds5-1400.webp"/> <img src="/assets/img/deepstream/ds5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> TensorRT Module </div>]]></content><author><name></name></author><category term="Nvidia-Tao"/><category term="SoftwareTools"/><summary type="html"><![CDATA[Using Nvidia Tools for Neural Networks]]></summary></entry><entry><title type="html">Introduction to Pybullet</title><link href="https://saisritejakuppa.github.io/blog/2022/PyBulletTutorial/" rel="alternate" type="text/html" title="Introduction to Pybullet"/><published>2022-09-22T16:40:16+00:00</published><updated>2022-09-22T16:40:16+00:00</updated><id>https://saisritejakuppa.github.io/blog/2022/PyBulletTutorial</id><content type="html" xml:base="https://saisritejakuppa.github.io/blog/2022/PyBulletTutorial/"><![CDATA[<h1 id="pybullet-robots-and-cameras">Pybullet: Robots and Cameras</h1> <p>Welcome to the tutorial on using PyBullet, the physics engine for simulating rigid body dynamics. In this blog post, we will be diving into the basics of PyBullet and how to use it to simulate physics in your own projects. Whether you‚Äôre a beginner or an experienced developer, this tutorial will provide you with the knowledge and tools you need to get started with PyBullet. So, let‚Äôs get started and learn how to create realistic physics simulations with PyBullet!</p> <p align="center"> <img src="https://user-images.githubusercontent.com/70435083/215378837-13b4e1a8-f2db-4aff-96ff-e76ea7639b80.gif" alt="ultrasoundgif" class="center"/> </p> <p align="center"> <a href="">UR5 robot interaction with a torid soft body.</a> </p> <p>In this blog, we will be utilizing a UR5 robot to demonstrate the capabilities of PyBullet. We will guide you through the process of loading the robot into the simulation and show you how to manipulate its movement. Additionally, we will also explore the use of cameras to view the robot from different angles, providing a more realistic representation of its motion</p> <p>URDF (Unified Robot Description Format) is a file format used to describe the physical structure and kinematics of a robot. It is an XML-based format that is used to define the robot‚Äôs links, joints, and sensors. The URDF file contains information about the robot‚Äôs geometry, mass properties, joint limits, and other parameters. It is used to define the robot‚Äôs model for physics simulation and visualization.</p> <p>URDF is widely used in robotics, it is used in popular robot simulators like Gazebo and PyBullet, as well as in many robot operating systems like ROS. URDF files can be created manually or generated from CAD models using various tools like xacro, URDF exporter from SolidWorks, Inventor, etc. The URDF file can be loaded into a robot simulator, and the robot‚Äôs model can be used for physics simulation, motion planning, and visualization.</p> <p>PyBullet, a physics simulation engine, also provides support for simulating soft-body dynamics. Soft-bodies are objects that can bend, stretch, and deform, unlike rigid bodies that maintain a fixed shape. Examples of soft-bodies include fabrics, ropes, and other flexible materials.</p> <p>In PyBullet, soft-bodies are represented using the Bullet Soft Body library. This library allows users to create and simulate soft-bodies using a variety of methods, including cloth, rope, and mesh simulations. The library also provides several parameters that can be used to control the behavior of the soft-body, such as stiffness, damping, and mass.</p> <p>The user can create a soft-body using the p.createSoftBody function. This function takes several arguments such as the shape of the soft-body, its mass, and its collision shape. After creating the soft-body, the user can apply forces and torques to it, and the library will simulate its dynamics accordingly.</p> <p>One of the main advantages of using soft-bodies in PyBullet is the ability to create realistic simulations of flexible materials such as fabrics, ropes, and other flexible materials. This can be useful in many applications such as robotics, animation, and gaming.</p> <p align="center"> <img src="https://user-images.githubusercontent.com/70435083/215378959-46fa9d30-1bc1-4625-805b-d64a319fbf57.gif" alt="ultrasoundgif" class="center"/> </p> <p align="center"> <a href="">Interaction with the soft body.</a> </p> <p align="center"> <img src="https://user-images.githubusercontent.com/70435083/215380122-689f9c61-64c1-46ee-941c-426424f31743.JPG" alt="ultrasoundgif" class="center"/> <br/> <a href="">Close view of robot interacting with the toroid.</a> </p> <p>python code</p> <h2 id="install-the-dependencies-and-load-the-necessary-headers">Install the dependencies and load the necessary headers</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#install the dependencies and load the necessary headers
</span><span class="kn">import</span> <span class="n">pybullet</span> <span class="k">as</span> <span class="n">p</span>
<span class="kn">import</span> <span class="n">pybullet_data</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="c1">#connect to the physics server
</span><span class="n">p</span><span class="p">.</span><span class="nf">connect</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">DIRECT</span><span class="p">)</span>
<span class="c1">#allow to find the assets (URDF, obj, textures etc)
</span><span class="n">p</span><span class="p">.</span><span class="nf">setAdditionalSearchPath</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="nf">getDataPath</span><span class="p">())</span>

<span class="n">p</span><span class="p">.</span><span class="nf">setGravity</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">10</span><span class="p">)</span>

<span class="c1">#load the ground plane
</span><span class="n">planeId</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">loadURDF</span><span class="p">(</span><span class="sh">"</span><span class="s">plane.urdf</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="load-the-robot">Load the robot</h2> <p>startPos is a list of 3 floating-point numbers that represent the initial position of an object in 3D space. The values in the list correspond to the x, y, and z coordinates respectively. In this case, the initial position is set to the origin (0,0,0) which is the point (x,y,z) = (0,0,0) in the 3D space.</p> <p>startOrientation is a variable that holds the initial orientation of an object in the form of a quaternion. A quaternion is a 4D mathematical object that can be used to represent rotations in 3D space. In this case, startOrientation is set by calling the p.getQuaternionFromEuler function, which takes a list of 3 floating-point numbers representing the Euler angles (in radians) of the object‚Äôs initial orientation. The [0,0,0] passed as the argument corresponds to the yaw, pitch and roll of the object in that order.</p> <p>Together, startPos and startOrientation define the initial position and orientation of an object in the simulation. These values can be used to specify the initial state of an object when it is added to the simulation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#place the robot at the base position
</span><span class="n">startPos</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">startOrientation</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">getQuaternionFromEuler</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>

<span class="c1">#load the robot urdf file
</span><span class="n">boxId</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">loadURDF</span><span class="p">(</span><span class="sh">"</span><span class="s">/content/pybullet-works/notebooks/meshes/ur5.urdf</span><span class="sh">"</span><span class="p">,</span><span class="n">startPos</span><span class="p">,</span> <span class="n">startOrientation</span><span class="p">)</span>
</code></pre></div></div> <h2 id="adding-camera-to-the-scence">Adding camera to the scence</h2> <p>The variables pitch, roll, and yaw define the rotation of the camera in 3D space. pitch represents the rotation around the x-axis, roll represents the rotation around the y-axis, and yaw represents the rotation around the z-axis. In this case, the pitch is set to -10 degrees, which will make the camera look slightly downwards, the roll is set to 0, and the yaw is not defined.</p> <p>upAxisIndex is an integer variable that represents the up axis of the camera. This variable is used to specify which axis of the camera is pointing upwards. In this case, the value is set to 2, which corresponds to the z-axis.</p> <p>camDistance is a variable that represents the distance of the camera from the target position. In this case, the camera is 1.5 units away from the target position.</p> <p>pixelWidth and pixelHeight define the resolution of the image captured by the camera. In this case, the image will be 640 pixels wide and 480 pixels tall.</p> <p>nearPlane and farPlane define the near and far clipping planes of the camera, respectively. Objects closer than the near plane or farther than the far plane will not be visible in the captured image. In this case, the near plane is set to 0.01 units and the far plane is set to 100 units.</p> <p>fov is the field of view of the camera, measured in degrees. In this case, the field of view is set to 60 degrees.</p> <p>viewMatrix and projectionMatrix are 4x4 matrices that define the position and configuration of the camera. viewMatrix is computed using the p.computeViewMatrixFromYawPitchRoll function, which takes several arguments such as the target position, distance, yaw, pitch, roll, and up axis index of the camera. projectionMatrix is computed using the p.computeProjectionMatrixFOV function, which takes the field of view, aspect ratio, near and far clipping planes of the camera.</p> <p>Finally, the p.getCameraImage function is used to capture an image of the simulation. This function takes several arguments such as the width, height, viewMatrix, and projectionMatrix of the camera and returns an image in the form of an array.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>
<span class="n">camTargetPos</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">cameraUp</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">cameraPos</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">p</span><span class="p">.</span><span class="nf">setGravity</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">google.colab</span> <span class="kn">import</span> <span class="n">widgets</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">widgets</span><span class="p">.</span><span class="nc">Grid</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">yaw</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">yaw</span> <span class="o">+=</span> <span class="mi">60</span>
    <span class="k">with</span> <span class="n">grid</span><span class="p">.</span><span class="nf">output_to</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
      <span class="n">grid</span><span class="p">.</span><span class="nf">clear_cell</span><span class="p">()</span>
      <span class="n">pylab</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
      <span class="n">pitch</span> <span class="o">=</span> <span class="o">-</span><span class="mf">10.0</span>
      <span class="n">roll</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">upAxisIndex</span> <span class="o">=</span> <span class="mi">2</span>
      <span class="n">camDistance</span> <span class="o">=</span> <span class="mf">1.5</span>
      <span class="n">pixelWidth</span> <span class="o">=</span> <span class="mi">640</span>
      <span class="n">pixelHeight</span> <span class="o">=</span> <span class="mi">480</span>
      <span class="n">nearPlane</span> <span class="o">=</span> <span class="mf">0.01</span>
      <span class="n">farPlane</span> <span class="o">=</span> <span class="mi">100</span>
      <span class="n">fov</span> <span class="o">=</span> <span class="mi">60</span>
      <span class="n">viewMatrix</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">computeViewMatrixFromYawPitchRoll</span><span class="p">(</span><span class="n">camTargetPos</span><span class="p">,</span> <span class="n">camDistance</span><span class="p">,</span> <span class="n">yaw</span><span class="p">,</span> <span class="n">pitch</span><span class="p">,</span>
                                                                  <span class="n">roll</span><span class="p">,</span> <span class="n">upAxisIndex</span><span class="p">)</span>
      <span class="n">aspect</span> <span class="o">=</span> <span class="n">pixelWidth</span> <span class="o">/</span> <span class="n">pixelHeight</span>
      <span class="n">projectionMatrix</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">computeProjectionMatrixFOV</span><span class="p">(</span><span class="n">fov</span><span class="p">,</span> <span class="n">aspect</span><span class="p">,</span> <span class="n">nearPlane</span><span class="p">,</span> <span class="n">farPlane</span><span class="p">)</span>
          
      <span class="n">img_arr</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">getCameraImage</span><span class="p">(</span><span class="n">pixelWidth</span><span class="p">,</span><span class="n">pixelHeight</span><span class="p">,</span><span class="n">viewMatrix</span><span class="p">,</span><span class="n">projectionMatrix</span><span class="p">)</span>
      <span class="n">w</span> <span class="o">=</span> <span class="n">img_arr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1">#width of the image, in pixels
</span>      <span class="n">h</span> <span class="o">=</span> <span class="n">img_arr</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1">#height of the image, in pixels
</span>      <span class="n">rgb</span> <span class="o">=</span> <span class="n">img_arr</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="c1">#color data RGB
</span>      <span class="n">dep</span> <span class="o">=</span> <span class="n">img_arr</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>  <span class="c1">#depth data
</span>      <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">w=</span><span class="sh">"</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="sh">"</span><span class="s">h=</span><span class="sh">"</span><span class="p">,</span><span class="n">h</span><span class="p">)</span>
      <span class="n">np_img_arr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">rgb</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
      <span class="n">np_img_arr</span> <span class="o">=</span> <span class="n">np_img_arr</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="mf">255.</span><span class="p">)</span>
      <span class="n">pylab</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">np_img_arr</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">animated</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">pybullet</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p align="center"> <img src="https://user-images.githubusercontent.com/70435083/215380119-15fcf56b-8aaa-429a-854c-b58bb810fab4.JPG" alt="ultrasoundgif" class="center"/> <br/> <a href="">Robot in all views.</a> </p> <h2 id="create-an-animated-png">Create an animated png</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">numpngw</span>
<span class="kn">from</span> <span class="n">numpngw</span> <span class="kn">import</span> <span class="n">write_apng</span>
<span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>


<span class="n">frames</span><span class="o">=</span><span class="p">[]</span> <span class="c1">#frames to create animated png
</span><span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">60</span><span class="p">):</span>
    <span class="n">yaw</span> <span class="o">+=</span> <span class="mi">6</span>
    <span class="n">pitch</span> <span class="o">=</span> <span class="o">-</span><span class="mf">10.0</span>
    <span class="n">roll</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">upAxisIndex</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">camDistance</span> <span class="o">=</span> <span class="mf">1.5</span>
    <span class="n">pixelWidth</span> <span class="o">=</span> <span class="mi">320</span>
    <span class="n">pixelHeight</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="n">nearPlane</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="n">farPlane</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">fov</span> <span class="o">=</span> <span class="mi">60</span>
    <span class="n">viewMatrix</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">computeViewMatrixFromYawPitchRoll</span><span class="p">(</span><span class="n">camTargetPos</span><span class="p">,</span> <span class="n">camDistance</span><span class="p">,</span> <span class="n">yaw</span><span class="p">,</span> <span class="n">pitch</span><span class="p">,</span>
                                                                <span class="n">roll</span><span class="p">,</span> <span class="n">upAxisIndex</span><span class="p">)</span>
    <span class="n">aspect</span> <span class="o">=</span> <span class="n">pixelWidth</span> <span class="o">/</span> <span class="n">pixelHeight</span>
    <span class="n">projectionMatrix</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">computeProjectionMatrixFOV</span><span class="p">(</span><span class="n">fov</span><span class="p">,</span> <span class="n">aspect</span><span class="p">,</span> <span class="n">nearPlane</span><span class="p">,</span> <span class="n">farPlane</span><span class="p">)</span>
        
    <span class="n">img_arr</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">getCameraImage</span><span class="p">(</span><span class="n">pixelWidth</span><span class="p">,</span><span class="n">pixelHeight</span><span class="p">,</span><span class="n">viewMatrix</span><span class="p">,</span><span class="n">projectionMatrix</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">img_arr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1">#width of the image, in pixels
</span>    <span class="n">h</span> <span class="o">=</span> <span class="n">img_arr</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1">#height of the image, in pixels
</span>    <span class="n">rgb</span> <span class="o">=</span> <span class="n">img_arr</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="c1">#color data RGB
</span>    <span class="n">dep</span> <span class="o">=</span> <span class="n">img_arr</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>  <span class="c1">#depth data
</span>    <span class="c1">#print("w=",w,"h=",h)
</span>    <span class="n">np_img_arr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">rgb</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">frame</span> <span class="o">=</span> <span class="n">np_img_arr</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">frames</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">creating animated png, please about 5 seconds</span><span class="sh">"</span><span class="p">)</span>
<span class="o">%</span><span class="n">time</span> <span class="nf">write_apng</span><span class="p">(</span><span class="sh">"</span><span class="s">example6.png</span><span class="sh">"</span><span class="p">,</span> <span class="n">frames</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="o">%</span><span class="n">time</span> <span class="nc">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="sh">"</span><span class="s">example6.png</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p align="center"> <img src="https://user-images.githubusercontent.com/70435083/215378974-9468cbe5-50e2-49a3-8a74-4031634cc8f1.gif" alt="ultrasoundgif" class="center"/> </p> <p align="center"> <a href="">UR5 robot interaction with a torid soft body.</a> </p>]]></content><author><name></name></author><category term="Pybullet"/><category term="Robotics"/><summary type="html"><![CDATA[Pybullet tutorial, Load and View robots.]]></summary></entry><entry><title type="html">Deep Learning Diffusion - Code</title><link href="https://saisritejakuppa.github.io/blog/2022/DeepLearningDiffusion/" rel="alternate" type="text/html" title="Deep Learning Diffusion - Code"/><published>2022-08-22T16:40:16+00:00</published><updated>2022-08-22T16:40:16+00:00</updated><id>https://saisritejakuppa.github.io/blog/2022/DeepLearningDiffusion</id><content type="html" xml:base="https://saisritejakuppa.github.io/blog/2022/DeepLearningDiffusion/"><![CDATA[<h2 id="diffusion-vs-gan">Diffusion vs GAN</h2> <p>For image generation, we have been using GANs for a considerable time. However, training GANs can be challenging due to the need to control multiple loss functions and assign individual weights to them. To overcome this difficulty, we turn to diffusion models.</p> <p>Diffusion models have shown promising results in generating images with high fidelity and diversity, with a lower incidence of model collapse compared to GANs. Inspired by the impressive outputs of midjourney and DALL-E, I decided to build my own custom application using diffusion models.</p> <p>If you are new to this, and do not want to delve into the mathematical details, <a href="https://huggingface.co/docs/diffusers">diffusers</a> can be a useful starting point. They offer pipelines and pre-built components to experiment with. However, as a researcher who frequently reads papers, I find it important to understand the underlying math to achieve the best results.</p> <h2 id="the-diffusion-models">The Diffusion Models</h2> <p>Lets begin the journey. I am assuming you have some good grasp on deep learning tearms and moving with that in mind.</p> <p><em>Model Building</em> and <em>Training Process</em>, these are the only things you need to focus on if have already trained a simple CNN in pytorch. Lets start with each of them.</p> <h2 id="model-building">Model Building</h2> <p>In the deep learning world, signals can be of format audio, vision(image in 2d, 3d), text. These are my main focus.</p> <blockquote> <p>Any information is converted to tensor, these tensors from higher dimension brought down to lower dimension, these are called <em>Embeddings</em>. Its all about embeddings in the end of the day.</p> </blockquote> <p>I like images, so in today experiment we will start with generating new images without any condition. If you like text/ audio, find a way to convert it to tensors and try the experiments from the research as well.</p> <h3 id="core-blocks-of-diffusion-model">Core Blocks of Diffusion Model</h3> <p>The core blocks of the diffusion model:</p> <ol> <li>Downscaling Blocks</li> <li>Upscaling Blocks</li> <li>Residual Blocks</li> <li>Convolutional Blocks</li> <li>Attention Blocks</li> </ol> <p>Lets take an image 512 X 512 X 3, the number of parameters are 786432, doing computations on such high number is not possible unless we deal with lower dimensions. So, we need to downscale the image to lower dimensions. This is where the <em>downscaling blocks</em> come into the picture.</p> <p>Now what we do is take the lower dimension image( latent image ), add information to it and remove information from it. This is where the <em>residual blocks</em>, <em>convolutional</em> and <em>attention blocks</em> come into the picture.</p> <p>Convolutions picks the information from the neighbouring pixels and adds it to the current pixel. So it looks around a few neighbouring pixels. In attension, each pixel looks at all the other pixels in the entire feature map. So, it is a global operation. Thus the reason transformers are better than the convs. Residual connections helps during the backwards propagation for the flow of the gradients.</p> <p>The refined lower dimension image does consist of the information of the higher dimension image. So, we need to upscale the image to the original dimension. This is where the <em>upscaling blocks</em> come into the picture.</p> <p><br/></p> <hr/> <h3 id="convolutional-block">Convolutional Block</h3> <p>Convolution blocks helps to retain the information of the neighbouring pixels. This helps to presever from low freq to high freq information.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">mid_channels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">mid_channels</span><span class="p">:</span>
            <span class="n">mid_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">double_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">mid_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mid_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">mid_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">residual</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">gelu</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">double_conv</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">double_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p><br/></p> <hr/> <h3 id="downscaling-blocks">Downscaling Blocks</h3> <p>Downscaling block helps to reduce the dimension of the image. This is done by maxpooling and convolution. The maxpooling helps to reduce the dimension of the image. The convolution helps to retain the information of the neighbouring pixels.</p> <p>Take a focus on the variable t, but t is an embedding or extra information we are giving to the model. This can be any vector.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">Down</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">maxpool_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">emb_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
                <span class="n">emb_dim</span><span class="p">,</span>
                <span class="n">out_channels</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">maxpool_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">emb_layer</span><span class="p">(</span><span class="n">t</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">].</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">emb</span>
</code></pre></div></div> <p><br/></p> <hr/> <h3 id="upscaling-block">Upscaling Block</h3> <p>Upscaling block you can think of it as the reverse of the downscaling block. It helps to upscale the image to the original dimension.</p> <p>we have discussed about the resnet block, we didnt rather implement a whole block, we embedded into the code as a line, where we take information from the downscale and add the information to the upscale blocks. This helps to preserve the information of the higher dimensions of the feature maps and the backpropagation is easy as well.</p> <p>Take a focus on the variable t, but t is an embedding or extra information we are giving to the model. This can be any vector.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">Up</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">up</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">bilinear</span><span class="sh">"</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">emb_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
                <span class="n">emb_dim</span><span class="p">,</span>
                <span class="n">out_channels</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">skip_x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">skip_x</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">emb_layer</span><span class="p">(</span><span class="n">t</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">].</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">emb</span>
</code></pre></div></div> <p><br/></p> <hr/> <h3 id="self-attension-block">Self Attension Block</h3> <p>Self attention picks only important features and helps you to retain the the block things which are essential. There is self attention and cross attention. Self Attension is used to get important features from the images. Imagine now you want to cluster up two images, then you start looking for cross attenstion. The other one need not be an image alone, also text/audio. Tensors again.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">channels</span> <span class="o">=</span> <span class="n">channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        
        <span class="c1"># embed_dims, num_heads, 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">ln</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">([</span><span class="n">channels</span><span class="p">])</span>
        
        <span class="c1">#convert to req final shape
</span>        <span class="n">self</span><span class="p">.</span><span class="n">ff_self</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">([</span><span class="n">channels</span><span class="p">]),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">).</span><span class="nf">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>   <span class="c1">#[1,4,16,16] -&gt; [1,256,4]
</span>        <span class="n">x_ln</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                                                     <span class="c1">#[1,4,16,16] -&gt; [1,256,4]               
</span>        
        <span class="c1">#query, key, value -&gt; attn_output, attn_output_weights
</span>        <span class="n">attention_value</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mha</span><span class="p">(</span><span class="n">x_ln</span><span class="p">,</span> <span class="n">x_ln</span><span class="p">,</span> <span class="n">x_ln</span><span class="p">)</span>                       <span class="c1">#[1,256,4] -&gt; [1,256,4]
</span>                
        <span class="c1">#resnet connection
</span>        <span class="n">attention_value</span> <span class="o">=</span> <span class="n">attention_value</span> <span class="o">+</span> <span class="n">x</span>
        
        <span class="c1">#add the feed forward layer to get more features and attenuate the noise
</span>        <span class="n">attention_value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ff_self</span><span class="p">(</span><span class="n">attention_value</span><span class="p">)</span> <span class="o">+</span> <span class="n">attention_value</span>           <span class="c1">#[1,256,4] -&gt; [1,256,4]
</span>        
        <span class="k">return</span> <span class="n">attention_value</span><span class="p">.</span><span class="nf">swapaxes</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>  <span class="c1">#[1,256,4] -&gt; [1,4,16,16]
</span></code></pre></div></div> <p><br/></p> <hr/> <h3 id="unet-model">Unet Model</h3> <p>Unet is a model introduced intially for bio medical segmentation, but soon it spread in every domain. The unet consist of downscaling + bottleneck + upscaling blocks.</p> <p>There are skip connections from downsacling to upscaling blocks to preserve the information from the original image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">UNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">c_in</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c_out</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">time_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_dim</span> <span class="o">=</span> <span class="n">time_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">inc</span> <span class="o">=</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down1</span> <span class="o">=</span> <span class="nc">Down</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa1</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down2</span> <span class="o">=</span> <span class="nc">Down</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa2</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down3</span> <span class="o">=</span> <span class="nc">Down</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa3</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bot1</span> <span class="o">=</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bot2</span> <span class="o">=</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bot3</span> <span class="o">=</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up1</span> <span class="o">=</span> <span class="nc">Up</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa4</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up2</span> <span class="o">=</span> <span class="nc">Up</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa5</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up3</span> <span class="o">=</span> <span class="nc">Up</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa6</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">outc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">pos_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">channels</span><span class="p">):</span>
        <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span>
            <span class="mi">10000</span>
            <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">channels</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">pos_enc_a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">inv_freq</span><span class="p">)</span>
        <span class="n">pos_enc_b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">inv_freq</span><span class="p">)</span>
        <span class="n">pos_enc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">pos_enc_a</span><span class="p">,</span> <span class="n">pos_enc_b</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pos_enc</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">type</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pos_encoding</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">time_dim</span><span class="p">)</span>

        <span class="c1">#downsampling
</span>        <span class="n">x1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">inc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down1</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa1</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down2</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa2</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down3</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa3</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>

        <span class="c1">#bottleneck
</span>        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bot1</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bot2</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bot3</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>


        <span class="c1">#upscaling
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up1</span><span class="p">(</span><span class="n">x4</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa6</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">outc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p><strong>Positions Encoding block</strong></p> <p>The position encoding block helps as gudiance of the attension block to tell which pixels are close and which pixels are far for a certain pixel in the image. We use sine and cosine because of the reapeating frequency over a certain range. This position embeddings tells the location of the pixels, so that the attention block knows how can they be related to each other.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">pos_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">channels</span><span class="p">):</span>
        <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span>
            <span class="mi">10000</span>
            <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">channels</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">pos_enc_a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">inv_freq</span><span class="p">)</span>
        <span class="n">pos_enc_b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">inv_freq</span><span class="p">)</span>
        <span class="n">pos_enc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">pos_enc_a</span><span class="p">,</span> <span class="n">pos_enc_b</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pos_enc</span>
</code></pre></div></div> <p><br/></p> <hr/> <h2 id="model-training">Model Training</h2> <p>In Gans we just give noise and simply expect it to produce an image. But in this case we have to give the model the image and the time stamp. The time stamp tells us, how much noise it has to be added over time.</p> <p><strong>Forward Process: </strong> We take a really good image and with each time step we add noise(gaussian - normal distribution). As the time step increase we add more noise and the original image gets distorted completely.</p> <p>Check the image all the time before you add the noise and remove the noise.</p> <p>In the below code you can find that x is the image and t is the time step. We use certain maths as mentioned in the previous blog for math.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">noise_images</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">sqrt_alpha_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha_hat</span><span class="p">[</span><span class="n">t</span><span class="p">])[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
        <span class="n">sqrt_one_minus_alpha_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha_hat</span><span class="p">[</span><span class="n">t</span><span class="p">])[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
        <span class="n">∆ê</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sqrt_alpha_hat</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">sqrt_one_minus_alpha_hat</span> <span class="o">*</span> <span class="n">∆ê</span><span class="p">,</span> <span class="n">∆ê</span>
</code></pre></div></div> <p><strong>Backward Process: </strong></p> <p>Optimizer I am using is adam with a learning rate of 0.001. Loss function is a Mean sqaured error.</p> <p>Crux of the code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1">#get the time step
</span>    <span class="n">t</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">sample_timesteps</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>   <span class="c1">#[0,0.1,0.2,....1] but as int
</span>
    <span class="c1">#get the noise image
</span>    <span class="n">x_t</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">noise_images</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

    <span class="c1">#predict the noise
</span>    <span class="n">predicted_noise</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

    <span class="c1">#calculate the loss predicted noise and the original noise generated from algo
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="nf">mse</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>

    <span class="c1">#update the model parameters
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <p><br/></p> <hr/> <h2 id="model-testing">Model Testing</h2> <p>We randomly take noise from a normal distribution, iterative over the time steps, to get better and better image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Sampling </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s"> new images....</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">img_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">img_size</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">noise_steps</span><span class="p">)),</span> <span class="n">position</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
            <span class="n">t</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">i</span><span class="p">).</span><span class="nf">long</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">predicted_noise</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">[</span><span class="n">t</span><span class="p">][:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
            <span class="n">alpha_hat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha_hat</span><span class="p">[</span><span class="n">t</span><span class="p">][:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="p">[</span><span class="n">t</span><span class="p">][:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_hat</span><span class="p">)))</span> <span class="o">*</span> <span class="n">predicted_noise</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mi">255</span><span class="p">).</span><span class="nf">type</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="Diffusion"/><category term="DeepLearning"/><summary type="html"><![CDATA[Introduction to Diffusion in Deep Learning. We will be generating unconditional images.]]></summary></entry><entry><title type="html">Deep Learning Data Sources</title><link href="https://saisritejakuppa.github.io/blog/2022/DeepLearningTutorials/" rel="alternate" type="text/html" title="Deep Learning Data Sources"/><published>2022-08-22T16:40:16+00:00</published><updated>2022-08-22T16:40:16+00:00</updated><id>https://saisritejakuppa.github.io/blog/2022/DeepLearningTutorials</id><content type="html" xml:base="https://saisritejakuppa.github.io/blog/2022/DeepLearningTutorials/"><![CDATA[<p>Convolutional neural nets are the next thing in the deep learning world. They are the reason why deep learning is so popular in the field of computer vision. They are the reason why we can do so many things with images.</p> <h2 id="things-to-look-for-while-building-deep-learning-models">Things to look for while building deep learning models</h2> <ol> <li>Gather the dataset.</li> <li>Clean the dataset.</li> <li>Split the dataset into train, test and validation.</li> <li>Build the model.</li> <li>Train the model.</li> <li>Evaluate the model.</li> <li>Deploy the model.</li> <li>Monitor the model.</li> <li>Improve the model.</li> <li>Repeat the process. Process of building a CNN</li> </ol> <h2 id="getting-started-with-the-convolutional-tasks">Getting started with the convolutional tasks</h2> <ol> <li>Math for convolution - <a href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;pp=iAQB">Link</a></li> <li>Pytorch to implement a basic CNN - <a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html">Link</a></li> <li>Hyperparameters <ol> <li>Optimizers - <a href="https://www.youtube.com/watch?v=joKs2EJ9Z8w&amp;list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT&amp;index=35&amp;pp=iAQB">Math 5.x series</a></li> <li>Regularization - <a href="(https://www.youtube.com/watch?v=joKs2EJ9Z8w&amp;list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT&amp;index=35&amp;pp=iAQB)">Math 8.x, 9.x series</a></li> <li>Augmentations - <a href="https://albumentations.ai/docs/api_reference/augmentations/">Augmentations</a></li> </ol> </li> <li>Model Experimentations <ol> <li>wandb - <a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb">Link</a></li> <li>comet - <a href="https://colab.research.google.com/github/comet-ml/comet-examples/blob/master/integrations/model-training/pytorch/notebooks/Comet_and_Pytorch.ipynb">Link</a></li> </ol> </li> <li>Model Interpretability - <a href="https://captum.ai/tutorials/Resnet_TorchVision_Interpret">Link</a></li> <li>Deploy the model - <a href="https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html">Link</a></li> <li>Monitor the model - <a href="https://neptune.ai/blog/how-to-monitor-your-models-in-production-guide">Link</a></li> </ol> <h2 id="image-processing-blogs">Image Processing Blogs</h2> <ol> <li><a href="https://medium.com/@saisritejakuppa/image-processing-chapter-1-effc5780e2f6">Chapter 1</a></li> <li><a href="https://medium.com/@saisritejakuppa/image-processing-chapter-2-6dd461bb1601">Chapter 2</a></li> <li><a href="https://medium.com/@saisritejakuppa/image-processing-chapter-3-1b63c3b9fc14">Chapter 3</a></li> </ol> <h2 id="deep-learning-talks">Deep Learning Talks</h2> <ol> <li><a href="https://medium.com/@saisritejakuppa/using-images-to-train-large-langauge-models-597ecd10ebb6">Using Images to train LLM models.</a></li> <li><a href="https://medium.com/@saisritejakuppa/tranfer-learning-from-the-nature-1541095a196a">Transfer Learning from nature.</a></li> <li><a href="https://medium.com/@saisritejakuppa/underwater-robot-navigation-727f7deac61a">Underwater robot navigation.</a></li> <li><a href="https://medium.com/@saisritejakuppa/model-training-with-zero-data-3b1bbfc6dfbe">Model Training with zero data.</a></li> </ol>]]></content><author><name></name></author><category term="CNN"/><category term="DeepLearning"/><summary type="html"><![CDATA[Convolutional Neural Networks in Vision]]></summary></entry><entry><title type="html">Solving Grand Challenges in Video Analytics</title><link href="https://saisritejakuppa.github.io/blog/2022/NvidiaGTCDeepstream/" rel="alternate" type="text/html" title="Solving Grand Challenges in Video Analytics"/><published>2022-08-22T16:40:16+00:00</published><updated>2022-08-22T16:40:16+00:00</updated><id>https://saisritejakuppa.github.io/blog/2022/NvidiaGTCDeepstream</id><content type="html" xml:base="https://saisritejakuppa.github.io/blog/2022/NvidiaGTCDeepstream/"><![CDATA[<p>Alright now you know how to train a deep learning model, what‚Äôs next. You wanna deploy it and check it how its performing. If its performing bad, how to proceed further. If its performing good, how to scale it.</p> <p>In this blog we are gonna deal with all these stuff about deployment and model retraining for a high accuracy.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Place where you wanted to deploy your model usally. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%202-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%202-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%202-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%202.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%203-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%203-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%203-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%203.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Deploying the model in Deepstream. </div> <hr/> <h2 id="deepstream">Deepstream</h2> <p>The RTSP is a steam of video frames that are given to the video management software and AI models for inferences. Once the inference is done on the image, the database is stored and the inferences are pushed to custom analytical dashboard. Usually we have mutliple source of cameras and we build a pipeline for each of them. This can work for a small scale but when we have a large scale of cameras, this is not a good approach. We need better apporaches to handle this. If we can combine output from multiple cameras we can even get better results for applications like traffic management, crowd management etc.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%201-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%201-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%201-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%201.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Current Methods. </div> <hr/> <h2 id="features-in-upgraded-deepstream">Features in Upgraded Deepstream</h2> <p>Now we have a warehouse where a lot of people keep moving in it. One of the core application is to track and detect the person all the time.</p> <p>A detector module helps to draw a bouding boxes around a person and the tracker traces the path of the person. We find these modules often in any system. The tracker is used to check if a person enters an unauthorized area.</p> <p>Now image a person moves from one room to another, since there is a switch of a person in cam 1 to 2, a new tracker is allocated to same person, which is not what we wanted. In such situations we need to have a multi camera tracking system.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%204-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%204-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%204-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%204.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%205-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%205-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%205-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%205.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Deploying the model in Deepstream. </div> <h2 id="challenges-in-multiple-tracking-system">Challenges in Multiple Tracking System</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%206-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%206-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%206-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%206.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Challenges in Multiple Tracking System. </div> <h2 id="generating-cg-dataset-and-training-omniverse-">Generating CG Dataset and Training( Omniverse )</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2015-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2015-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2015-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2015.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> CG dataset Creation + Model Training. </div> <h2 id="bulding-the-cg-dataset">Bulding the CG Dataset</h2> <p>One the challenges in training a model is to get a good datasets. We usually do have the real datasets which is blurry. The other problem is, when it comes to things like tracking the ground truth is not available. Annotating such tasks is very difficult. These are the reasons why we need to approach for CG dataset. Omniverse gives you a set of predefined enviroments to deal with. We can use these environments to generate the dataset.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%209-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%209-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%209-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%209.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2014-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2014-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2014-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2014.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Nvidia Omniverse, place to built virtual env. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2010-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2010-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2010-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2010.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2011-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2011-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2011-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2011.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Building a digital twin of the warehouse. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2012-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2012-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2012-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2012.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Building a Virtual Humans in warehouse. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2013-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2013-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2013-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2013.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Output of CCTV photages generated from CG environment. </div> <h2 id="training-the-neural-nets">Training the neural nets.</h2> <p>In further training, along with the real data we can use the CG dataset for training the neural network. This will help the model to learn the features of the object in a better way. Also there will be any mistakes in the dataset given by the CG.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%208-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%208-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%208-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%208.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> New training pipeline. </div> <p>RE - ID is a process where the model is used to detect the same person in multiple cameras. This is done by using the features of the person. The features are extracted from the person and stored in a database. When a person is detected in a new camera, the features are extracted and compared with the database. If the features are matched, then the person is the same.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2016-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2016-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2016-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2016.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2017-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2017-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2017-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2017.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Re-ID for person tracking. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2021-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2021-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2021-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2021.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Performance on multiple machines. </div> <h2 id="building-multicamera-perception-system">Building Multicamera perception system</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%207-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%207-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%207-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%207.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Multiple camera perception system. </div> <p>We need to upgrade the system from single camera to multiple camera. This is done by using the multiple camera perception system. This system is used to detect the objects in multiple cameras.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2018-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2018-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2018-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2018.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Multiple camera modules. </div> <p>The multi tracking system have:</p> <ol> <li> <p>Pixel to Physical mapping: This is used to map the pixel to the physical location of the object. This is done by using the camera calibration.</p> </li> <li> <p>Behavioural analysis: This is used to analyse the behaviour of the object. This is done by using the object detection and tracking.</p> </li> <li> <p>Matching Process: This is used to match the object in multiple cameras. This is done by using the Re-ID.</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2019-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2019-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2019-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2019.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Multiple camera modules. </div> <h1 id="microservices-perception-data-storage-dashboard">Microservices( Perception, Data storage, Dashboard)</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2022-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2022-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2022-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2022.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Perception module. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2023-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2023-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2023-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2023.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Behaviour module. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2024-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2024-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2024-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2024.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Storage module. </div> <h2 id="metrapolis">Metrapolis</h2> <p>This is a platform where you can build all the things from end to end. Yet, omniverse is so expensive and deepstream doesnt have a great document.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2026-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2026-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2026-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2026.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2027-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2027-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Solving_Grand_Challenge/Untitled%2027-1400.webp"/> <img src="/assets/img/Solving_Grand_Challenge/Untitled%2027.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Metrapolis. </div>]]></content><author><name></name></author><category term="DeepStream"/><category term="SoftwareTools"/><summary type="html"><![CDATA[Embeddings]]></summary></entry></feed>