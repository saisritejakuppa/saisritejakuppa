<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Deep Learning Diffusion - Code | Sai Sri Teja K</title> <meta name="author" content="Sai Sri Teja K"> <meta name="description" content="Introduction to Diffusion in Deep Learning. We will be generating unconditional images."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img//assets/img/logo.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://saisritejakuppa.github.io/blog/2022/DeepLearningDiffusion/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sai Sri Teja </span>K</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Deep Learning Diffusion - Code</h1> <p class="post-meta">August 22, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/deeplearning"> <i class="fas fa-hashtag fa-sm"></i> DeepLearning</a>     ·   <a href="/blog/category/diffusion"> <i class="fas fa-tag fa-sm"></i> Diffusion</a>   </p> </header> <article class="post-content"> <h2 id="diffusion-vs-gan">Diffusion vs GAN</h2> <p>For image generation, we have been using GANs for a considerable time. However, training GANs can be challenging due to the need to control multiple loss functions and assign individual weights to them. To overcome this difficulty, we turn to diffusion models.</p> <p>Diffusion models have shown promising results in generating images with high fidelity and diversity, with a lower incidence of model collapse compared to GANs. Inspired by the impressive outputs of midjourney and DALL-E, I decided to build my own custom application using diffusion models.</p> <p>If you are new to this, and do not want to delve into the mathematical details, <a href="https://huggingface.co/docs/diffusers" rel="external nofollow noopener" target="_blank">diffusers</a> can be a useful starting point. They offer pipelines and pre-built components to experiment with. However, as a researcher who frequently reads papers, I find it important to understand the underlying math to achieve the best results.</p> <h2 id="the-diffusion-models">The Diffusion Models</h2> <p>Lets begin the journey. I am assuming you have some good grasp on deep learning tearms and moving with that in mind.</p> <p><em>Model Building</em> and <em>Training Process</em>, these are the only things you need to focus on if have already trained a simple CNN in pytorch. Lets start with each of them.</p> <h2 id="model-building">Model Building</h2> <p>In the deep learning world, signals can be of format audio, vision(image in 2d, 3d), text. These are my main focus.</p> <blockquote> <p>Any information is converted to tensor, these tensors from higher dimension brought down to lower dimension, these are called <em>Embeddings</em>. Its all about embeddings in the end of the day.</p> </blockquote> <p>I like images, so in today experiment we will start with generating new images without any condition. If you like text/ audio, find a way to convert it to tensors and try the experiments from the research as well.</p> <h3 id="core-blocks-of-diffusion-model">Core Blocks of Diffusion Model</h3> <p>The core blocks of the diffusion model:</p> <ol> <li>Downscaling Blocks</li> <li>Upscaling Blocks</li> <li>Residual Blocks</li> <li>Convolutional Blocks</li> <li>Attention Blocks</li> </ol> <p>Lets take an image 512 X 512 X 3, the number of parameters are 786432, doing computations on such high number is not possible unless we deal with lower dimensions. So, we need to downscale the image to lower dimensions. This is where the <em>downscaling blocks</em> come into the picture.</p> <p>Now what we do is take the lower dimension image( latent image ), add information to it and remove information from it. This is where the <em>residual blocks</em>, <em>convolutional</em> and <em>attention blocks</em> come into the picture.</p> <p>Convolutions picks the information from the neighbouring pixels and adds it to the current pixel. So it looks around a few neighbouring pixels. In attension, each pixel looks at all the other pixels in the entire feature map. So, it is a global operation. Thus the reason transformers are better than the convs. Residual connections helps during the backwards propagation for the flow of the gradients.</p> <p>The refined lower dimension image does consist of the information of the higher dimension image. So, we need to upscale the image to the original dimension. This is where the <em>upscaling blocks</em> come into the picture.</p> <p><br></p> <hr> <h3 id="convolutional-block">Convolutional Block</h3> <p>Convolution blocks helps to retain the information of the neighbouring pixels. This helps to presever from low freq to high freq information.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">mid_channels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">mid_channels</span><span class="p">:</span>
            <span class="n">mid_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">double_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">mid_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mid_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">mid_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">residual</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="nf">gelu</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">double_conv</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">double_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p><br></p> <hr> <h3 id="downscaling-blocks">Downscaling Blocks</h3> <p>Downscaling block helps to reduce the dimension of the image. This is done by maxpooling and convolution. The maxpooling helps to reduce the dimension of the image. The convolution helps to retain the information of the neighbouring pixels.</p> <p>Take a focus on the variable t, but t is an embedding or extra information we are giving to the model. This can be any vector.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">Down</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">maxpool_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">emb_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
                <span class="n">emb_dim</span><span class="p">,</span>
                <span class="n">out_channels</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">maxpool_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">emb_layer</span><span class="p">(</span><span class="n">t</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">].</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">emb</span>
</code></pre></div></div> <p><br></p> <hr> <h3 id="upscaling-block">Upscaling Block</h3> <p>Upscaling block you can think of it as the reverse of the downscaling block. It helps to upscale the image to the original dimension.</p> <p>we have discussed about the resnet block, we didnt rather implement a whole block, we embedded into the code as a line, where we take information from the downscale and add the information to the upscale blocks. This helps to preserve the information of the higher dimensions of the feature maps and the backpropagation is easy as well.</p> <p>Take a focus on the variable t, but t is an embedding or extra information we are giving to the model. This can be any vector.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">Up</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">up</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">bilinear</span><span class="sh">"</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">emb_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
                <span class="n">emb_dim</span><span class="p">,</span>
                <span class="n">out_channels</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">skip_x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">skip_x</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">emb_layer</span><span class="p">(</span><span class="n">t</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">].</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">emb</span>
</code></pre></div></div> <p><br></p> <hr> <h3 id="self-attension-block">Self Attension Block</h3> <p>Self attention picks only important features and helps you to retain the the block things which are essential. There is self attention and cross attention. Self Attension is used to get important features from the images. Imagine now you want to cluster up two images, then you start looking for cross attenstion. The other one need not be an image alone, also text/audio. Tensors again.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">channels</span> <span class="o">=</span> <span class="n">channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
        
        <span class="c1"># embed_dims, num_heads, 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MultiheadAttention</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">ln</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">([</span><span class="n">channels</span><span class="p">])</span>
        
        <span class="c1">#convert to req final shape
</span>        <span class="n">self</span><span class="p">.</span><span class="n">ff_self</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LayerNorm</span><span class="p">([</span><span class="n">channels</span><span class="p">]),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">).</span><span class="nf">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>   <span class="c1">#[1,4,16,16] -&gt; [1,256,4]
</span>        <span class="n">x_ln</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ln</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                                                     <span class="c1">#[1,4,16,16] -&gt; [1,256,4]               
</span>        
        <span class="c1">#query, key, value -&gt; attn_output, attn_output_weights
</span>        <span class="n">attention_value</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mha</span><span class="p">(</span><span class="n">x_ln</span><span class="p">,</span> <span class="n">x_ln</span><span class="p">,</span> <span class="n">x_ln</span><span class="p">)</span>                       <span class="c1">#[1,256,4] -&gt; [1,256,4]
</span>                
        <span class="c1">#resnet connection
</span>        <span class="n">attention_value</span> <span class="o">=</span> <span class="n">attention_value</span> <span class="o">+</span> <span class="n">x</span>
        
        <span class="c1">#add the feed forward layer to get more features and attenuate the noise
</span>        <span class="n">attention_value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ff_self</span><span class="p">(</span><span class="n">attention_value</span><span class="p">)</span> <span class="o">+</span> <span class="n">attention_value</span>           <span class="c1">#[1,256,4] -&gt; [1,256,4]
</span>        
        <span class="k">return</span> <span class="n">attention_value</span><span class="p">.</span><span class="nf">swapaxes</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">channels</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>  <span class="c1">#[1,256,4] -&gt; [1,4,16,16]
</span></code></pre></div></div> <p><br></p> <hr> <h3 id="unet-model">Unet Model</h3> <p>Unet is a model introduced intially for bio medical segmentation, but soon it spread in every domain. The unet consist of downscaling + bottleneck + upscaling blocks.</p> <p>There are skip connections from downsacling to upscaling blocks to preserve the information from the original image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">UNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">c_in</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c_out</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">time_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_dim</span> <span class="o">=</span> <span class="n">time_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">inc</span> <span class="o">=</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down1</span> <span class="o">=</span> <span class="nc">Down</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa1</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down2</span> <span class="o">=</span> <span class="nc">Down</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa2</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">down3</span> <span class="o">=</span> <span class="nc">Down</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa3</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bot1</span> <span class="o">=</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bot2</span> <span class="o">=</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bot3</span> <span class="o">=</span> <span class="nc">DoubleConv</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up1</span> <span class="o">=</span> <span class="nc">Up</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa4</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up2</span> <span class="o">=</span> <span class="nc">Up</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa5</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">up3</span> <span class="o">=</span> <span class="nc">Up</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sa6</span> <span class="o">=</span> <span class="nc">SelfAttention</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">outc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">pos_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">channels</span><span class="p">):</span>
        <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span>
            <span class="mi">10000</span>
            <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">channels</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">pos_enc_a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">inv_freq</span><span class="p">)</span>
        <span class="n">pos_enc_b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">inv_freq</span><span class="p">)</span>
        <span class="n">pos_enc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">pos_enc_a</span><span class="p">,</span> <span class="n">pos_enc_b</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pos_enc</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">type</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pos_encoding</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">time_dim</span><span class="p">)</span>

        <span class="c1">#downsampling
</span>        <span class="n">x1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">inc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down1</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa1</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down2</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa2</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">down3</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa3</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>

        <span class="c1">#bottleneck
</span>        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bot1</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bot2</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bot3</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>


        <span class="c1">#upscaling
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up1</span><span class="p">(</span><span class="n">x4</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">up3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sa6</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">outc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p><strong>Positions Encoding block</strong></p> <p>The position encoding block helps as gudiance of the attension block to tell which pixels are close and which pixels are far for a certain pixel in the image. We use sine and cosine because of the reapeating frequency over a certain range. This position embeddings tells the location of the pixels, so that the attention block knows how can they be related to each other.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">pos_encoding</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">channels</span><span class="p">):</span>
        <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span>
            <span class="mi">10000</span>
            <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">channels</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">pos_enc_a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">inv_freq</span><span class="p">)</span>
        <span class="n">pos_enc_b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">channels</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">inv_freq</span><span class="p">)</span>
        <span class="n">pos_enc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">pos_enc_a</span><span class="p">,</span> <span class="n">pos_enc_b</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pos_enc</span>
</code></pre></div></div> <p><br></p> <hr> <h2 id="model-training">Model Training</h2> <p>In Gans we just give noise and simply expect it to produce an image. But in this case we have to give the model the image and the time stamp. The time stamp tells us, how much noise it has to be added over time.</p> <p><strong>Forward Process: </strong> We take a really good image and with each time step we add noise(gaussian - normal distribution). As the time step increase we add more noise and the original image gets distorted completely.</p> <p>Check the image all the time before you add the noise and remove the noise.</p> <p>In the below code you can find that x is the image and t is the time step. We use certain maths as mentioned in the previous blog for math.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">noise_images</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">sqrt_alpha_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha_hat</span><span class="p">[</span><span class="n">t</span><span class="p">])[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
        <span class="n">sqrt_one_minus_alpha_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha_hat</span><span class="p">[</span><span class="n">t</span><span class="p">])[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
        <span class="n">Ɛ</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sqrt_alpha_hat</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">sqrt_one_minus_alpha_hat</span> <span class="o">*</span> <span class="n">Ɛ</span><span class="p">,</span> <span class="n">Ɛ</span>
</code></pre></div></div> <p><strong>Backward Process: </strong></p> <p>Optimizer I am using is adam with a learning rate of 0.001. Loss function is a Mean sqaured error.</p> <p>Crux of the code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1">#get the time step
</span>    <span class="n">t</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">sample_timesteps</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>   <span class="c1">#[0,0.1,0.2,....1] but as int
</span>
    <span class="c1">#get the noise image
</span>    <span class="n">x_t</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="nf">noise_images</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

    <span class="c1">#predict the noise
</span>    <span class="n">predicted_noise</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

    <span class="c1">#calculate the loss predicted noise and the original noise generated from algo
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="nf">mse</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>

    <span class="c1">#update the model parameters
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <p><br></p> <hr> <h2 id="model-testing">Model Testing</h2> <p>We randomly take noise from a normal distribution, iterative over the time steps, to get better and better image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">logging</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Sampling </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s"> new images....</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">img_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">img_size</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">noise_steps</span><span class="p">)),</span> <span class="n">position</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
            <span class="n">t</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">i</span><span class="p">).</span><span class="nf">long</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">predicted_noise</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">[</span><span class="n">t</span><span class="p">][:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
            <span class="n">alpha_hat</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha_hat</span><span class="p">[</span><span class="n">t</span><span class="p">][:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="p">[</span><span class="n">t</span><span class="p">][:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_hat</span><span class="p">)))</span> <span class="o">*</span> <span class="n">predicted_noise</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mi">255</span><span class="p">).</span><span class="nf">type</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/DisfluencyDetection/">Disfluency detection using deep learning.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/NvidiaTools/">Nvidia Tools</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/NvidiaGTCDeepstream/">Solving Grand Challenges in Video Analytics</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/FriendVishnu/">My friend Vishnu.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/ECCV2024/">ECCV 2024</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Sai Sri Teja K. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/saisritejakuppa/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>